{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Lesson 2 : Finding Gaussian Process hyperparameters\n\nBelow some packages to import that will be used for this lesson\n\nCell bellow is here for avoiding scrolling when plot is create within ipython notebook"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%javascript\nIPython.OutputArea.prototype._should_scroll = function(lines){\n    return false;\n}",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "IPython.OutputArea.prototype._should_scroll = function(lines){\n    return false;\n}\n",
            "text/plain": "<IPython.core.display.Javascript object>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Classical package for manipulating\n# array, for plotting and interactiv plots.\nimport pylab as plt\nfrom matplotlib import gridspec\nimport numpy as np\nimport ipywidgets as widgets\nfrom ipywidgets import interact\nimport itertools\n\n# Gaussian processes from scikit-learn is used for this lesson.\n# Other packages exist (e.g. george) but for the courses I guess\n# it would be the best because a lot of people in desc are already\n# using scikit-learn. The only suggestion would be about how to fit\n# hyperparameters in a more efficient way, but it will be done with\n# an other packages also broadly used within DESC, which is TreeCorr.\nfrom sklearn import gaussian_process as skl_gp\nfrom sklearn.gaussian_process.kernels import Kernel\n\n# Special implemetation of anisotropic squarred exponential kernel\n# in scikit-learn. Not implemented in scikit-learn originally.\nimport sys\nsys.path.append('/home/nbuser/project')\nfrom kernel import AnisotropicRBF\n\n# treecorr is a package to compute 2-point correlation function.\n# it will be use as an alternative way of Maximum Likelihood described \n# in Rasmussen & Williams 2006 to estimate hyperparameters.\ntry:\n    import treecorr\nexcept:\n    !pip install treecorr\n    import treecorr\n    \ntry:\n    import emcee\nexcept:    \n    !pip install emcee\n    import emcee\n\ntry:\n    import corner\nexcept:    \n    !pip install corner\n    import corner\n\n# Some import trickery to get all subclasses of sklearn.gaussian_process.kernels.Kernel\n# into the local namespace without doing \"from sklearn.gaussian_process.kernels import *\"\n# and without importing them all manually. Originally developped by Josh Meyers within Piff.\n# Example:\n# kernel = eval_kernel(\"RBF(1)\") instead of\n# kernel = sklearn.gaussian_process.kernels.RBF(1)\ndef eval_kernel(kernel):\n    def recurse_subclasses(cls):\n        out = []\n        for c in cls.__subclasses__():\n            out.append(c)\n            out.extend(recurse_subclasses(c))\n        return out\n    clses = recurse_subclasses(Kernel)\n    for cls in clses:\n        module = __import__(cls.__module__, globals(), locals(), cls)\n        execstr = \"{0} = module.{0}\".format(cls.__name__)\n        exec(execstr, globals(), locals())\n\n    from numpy import array\n\n    try:\n        k = eval(kernel)\n    except (KeyboardInterrupt, SystemExit):\n        raise\n    except Exception as e:  # pragma: no cover\n        raise RuntimeError(\"Failed to evaluate kernel string {0!r}.  \"\n                               \"Original exception: {1}\".format(kernel, e))\n\n    if type(k.theta) is property:\n        raise TypeError(\"String provided was not initialized properly\")\n    return k",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Exercice 5) Maximum Likelihood search of best hyperparameters / kernel (example in 1D):"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "##########################################################################################\n# EXERCICE 5: Maximum Likelihood search of best hyperparameters / kernel (example in 1D) #\n##########################################################################################\n\ndef log_likelihood(param, kernel_type=\"RBF\"):\n    if param[1] <=0:\n        return -np.inf\n    else:\n        Kernel = \"%f * %s(%f)\"%((param[0]**2, kernel_type, param[1]))\n        Kernel = eval_kernel(Kernel)\n    \n        gp = skl_gp.GaussianProcessRegressor(kernel=Kernel, alpha=y_err,\n                                             optimizer=None,\n                                             normalize_y=None)\n        gp.fit(x,y)\n        log_L = gp.log_marginal_likelihood()\n        return log_L\n\n\ndef mcmc_hyperparameters_search(run_mcmc=False):\n    if run_mcmc:\n        p0 = [1., 0.5]\n        np.random.seed(42)\n        ndim, nwalkers = len(p0), 100\n        pos = [p0 + 1e-4*np.random.randn(ndim) for i in range(nwalkers)]\n\n        sampler = emcee.EnsembleSampler(nwalkers, ndim, log_likelihood)\n        sampler.run_mcmc(pos, 600)\n        LABEL = ['$\\sigma$','$l$']\n        for j in range(ndim):\n            plt.figure()\n            for i in range(nwalkers):\n                plt.plot(sampler.chain[i,:,j],'k', alpha=0.1)\n            plt.ylabel(LABEL[j], fontsize=20)\n\n        samples = sampler.chain[:, 60:, :].reshape((-1, ndim))\n    \n        fig = corner.corner(samples, labels=LABEL,\n                            levels=(0.68, 0.95))\n        return samples\n\ndata = np.loadtxt('data/data_1d_grf.txt')\nx = data[:,0].reshape((len(data[:,0]),1))\ny = data[:,1]\ny_err = data[:,2]\n\n\ndef gp_regression(x, new_x, y, kernel, y_err=None):\n    \n    if y_err is None:\n        y_err =np.ones_like(y) *1e-10\n    \n    gp = skl_gp.GaussianProcessRegressor(kernel=kernel, alpha=y_err,\n                                         optimizer=None,\n                                         normalize_y=None)\n    gp.fit(x,y)\n    y_predict, y_std = gp.predict(new_x, return_std=True)\n    return y_predict, y_std\n\n@interact(sigma = widgets.FloatSlider(value=1.2, min=0.75, max=2.5, step=0.01, description='$\\sigma$:',\n          disabled=False,\n          continuous_update=False,\n          orientation='horizontal',\n          readout=True,\n          readout_format='.2f'), \n          l = widgets.FloatSlider(value=0.6, min=0.4, max=1.5, step=0.01, description='$l$:',\n          disabled=False,\n          continuous_update=False,\n          orientation='horizontal',\n          readout=True,\n          readout_format='.2f'),\n          kernel = widgets.Dropdown(options=['RBF', 'Matern'],\n                                  value='RBF',\n                                  description='Kernel:',\n                                  disabled=False,))\ndef plot_samples(sigma, l, kernel):\n    \n    new_x = np.linspace(-24,24, 400).reshape((400,1))\n    Kernel = \"%f * %s(%f)\"%((sigma**2, kernel, l))\n    Kernel = eval_kernel(Kernel)\n    y_pred, y_std = gp_regression(x, new_x, y, Kernel, y_err=y_err)\n\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1.5, 1])\n    plt.figure(figsize=(20,8))\n    plt.subplot(gs[0])\n    \n    # Data\n    plt.scatter(x, y, c='b', label = 'data')\n    plt.errorbar(x, y, linestyle='', yerr=y_err, ecolor='b', \n                 alpha=0.7,marker='.',zorder=0)\n    \n    # GP prediction\n    plt.plot(new_x, y_pred, 'r', lw =3, label = 'GP prediction')\n    plt.fill_between(new_x.T[0], y_pred-y_std, y_pred+y_std, color='r', alpha=0.3)\n    \n    plt.plot(new_x, np.zeros_like(new_x),'k--')\n    plt.xlim(-24,24)\n    plt.ylim(-3.,3.)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.xlabel('X', fontsize=20)\n    plt.ylabel('Y', fontsize=20)\n    plt.legend(fontsize=18)\n    \n    plt.subplot(gs[1])\n    \n    distance = np.linspace(0, 2, 40)\n    coord = np.array([distance, np.zeros_like(distance)]).T\n    pcf = Kernel.__call__(coord, Y=np.zeros_like(coord))[:,0]\n    \n    plt.plot(distance, pcf, 'k', lw=3)\n    \n    plt.ylim(0, 2.5**2)\n    plt.xlim(0, 2)\n    plt.ylabel('$\\\\xi(|x_i-x_j|)$', fontsize=20)\n    plt.xlabel('$|x_i-x_j|$', fontsize=20)\n    plt.title('Used correlation function (%s)'%(kernel), fontsize=16)\n    \n    samples = np.loadtxt('data/data_1d_grf_mcmc_likelihood_sampling_%s.txt'%(kernel))\n    fig = corner.corner(samples, labels=['$\\sigma$','$l$'],\n                        truths=[sigma, l, ], levels=(0.68, 0.95))\n    fig.suptitle('Kernel type: ' + kernel + ', $\\log$ likelihood = %.2f'%(log_likelihood([sigma, l], kernel_type=kernel)))",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73cf8976d3c94c23aa20bc804c92f3f9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "interactive(children=(FloatSlider(value=1.2, continuous_update=False, description='$\\\\sigma$:', max=2.5, min=0â€¦"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Exercice 6) 2-point correlation function search of best hyperparameters / kernel (example in 1D) :"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "##########################################################################################\n# EXERCICE 6: Maximum Likelihood search of best hyperparameters / kernel (example in 1D) #\n##########################################################################################\n\ndata = np.loadtxt('data/data_1d_grf_4000_points.txt')\nx = data[:,0].reshape((len(data[:,0]),1))\ny = data[:,1]\ny_err = data[:,2]\n\nnp.random.seed(42)\nFilter = np.random.choice([True, False, False, False, False], size=len(y))\n\ncat = treecorr.Catalog(x=x[:,0], y=np.zeros_like(x[:,0]), k=(y-np.mean(y)), w=1./y_err**2)\nkk = treecorr.KKCorrelation(min_sep=0.05, max_sep=1.5, nbins=15.)\nkk.process(cat)\ndelta_distance = kk.meanr\nxi = kk.xi\n\n\ndef gp_regression(x, new_x, y, kernel, y_err=None):\n    \n    if y_err is None:\n        y_err =np.ones_like(y) *1e-10\n    \n    gp = skl_gp.GaussianProcessRegressor(kernel=kernel, alpha=y_err,\n                                         optimizer=None,\n                                         normalize_y=None)\n    gp.fit(x,y)\n    y_predict, y_std = gp.predict(new_x, return_std=True)\n    return y_predict, y_std\n\n@interact(sigma = widgets.FloatSlider(value=1.2, min=0.75, max=2.5, step=0.01, description='$\\sigma$:',\n          disabled=False,\n          continuous_update=False,\n          orientation='horizontal',\n          readout=True,\n          readout_format='.2f'), \n          l = widgets.FloatSlider(value=0.6, min=0.4, max=1.5, step=0.01, description='$l$:',\n          disabled=False,\n          continuous_update=False,\n          orientation='horizontal',\n          readout=True,\n          readout_format='.2f'),\n          kernel = widgets.Dropdown(options=['RBF', 'Matern'],\n                                  value='RBF',\n                                  description='Kernel:',\n                                  disabled=False,))\ndef plot_samples(sigma, l, kernel):\n\n    y_reduce = y[Filter]\n    x_reduce = x[Filter]\n    y_err_reduce = y_err[Filter]\n    \n    new_x = np.linspace(-55, 55, 500).reshape((500,1))\n    Kernel = \"%f * %s(%f)\"%((sigma**2, kernel, l))\n    Kernel = eval_kernel(Kernel)\n    y_pred, y_std = gp_regression(x_reduce, new_x, y_reduce, \n                                  Kernel, y_err=y_err_reduce)\n\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1.5, 1])\n    plt.figure(figsize=(20,8))\n    plt.subplot(gs[0])\n    \n    # Data\n    plt.scatter(x, y, c='b', label = 'data')\n    plt.errorbar(x, y, linestyle='', yerr=y_err, ecolor='b', \n                 alpha=0.7,marker='.',zorder=0)\n    \n    # GP prediction\n    plt.plot(new_x, y_pred, 'r', lw =3, label = 'GP prediction')\n    plt.fill_between(new_x.T[0], y_pred-y_std, y_pred+y_std, color='r', alpha=0.3)\n    \n    plt.plot(new_x, np.zeros_like(new_x),'k--')\n    plt.xlim(-55, 55)\n    plt.ylim(-3.,3.)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.xlabel('X', fontsize=20)\n    plt.ylabel('Y', fontsize=20)\n    plt.legend(fontsize=18)\n    \n    plt.subplot(gs[1])\n    \n    distance = np.linspace(0, 2, 40)\n    coord = np.array([distance, np.zeros_like(distance)]).T\n    pcf = Kernel.__call__(coord, Y=np.zeros_like(coord))[:,0]\n    \n    plt.plot(distance, pcf, 'k', lw=3, label=\"Used correlation function\")\n    plt.scatter(delta_distance, xi, c='b', s=80, label=\"Measured 2-point correlation function\")\n    \n    plt.ylim(0, 2.)\n    plt.xlim(0, 2)\n    plt.ylabel('$\\\\xi(|x_i-x_j|)$', fontsize=20)\n    plt.xlabel('$|x_i-x_j|$', fontsize=20)\n    plt.legend(fontsize=14)\n    plt.title('Used correlation function (%s)'%(kernel), fontsize=16)",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7fb2631fed14802a3165240bc78117d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "interactive(children=(FloatSlider(value=1.2, continuous_update=False, description='$\\\\sigma$:', max=2.5, min=0â€¦"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}